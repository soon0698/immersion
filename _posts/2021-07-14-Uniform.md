---
toc: true
comments: true
layout: post
description: 딥러닝 압축 분야에서 엔트로피 가능도(entropy likelihood)를 계산하기 위해, 본래 미분이 불가능했던 이산적(discrete) 데이터를 연속적인(continuous) 근사를 통해 gradient를 계산할 수 있도록 하는 Uniform additive noise 방법에 대하여 서술한다.
categories: [markdown]
title: Uniform additive noise는 무엇이고, 어떻게 동작하는가?
---


## Introduction
- Data Compression에 관한 내용
- Shanon 표현법
- 엔트로피 최소화
- 기존 코덱 방법 동작
- 코딩 (CABAC) 등
- 'zero gradient'

## Traditional Video Codec
- 주파수 도메인 변환
- 양자화 & 라운딩
- QP 값
- RD-cost tradeoff (Perception tradeoff)
- RD plot

## Optimized End-to-end
- Review (GDN)
- Review (Uniform additive noise)
- Continuously-relaxed loss function
- '당황스럽게 잘 동작'
- 그림

## Deep Compression
STE

## Scalable Model Compression 

"Linear filters were parameterized using their discrete cosine transform (DCT) coefficients.
We found this to be slightly more effective in speeding up the convergence than discrete
Fourier transform (DFT) parameterization - ICLR'17"

(2부?)
## Implementation (code)
- tensorflow-compression
- CompressAI

## 코드의 동작법 (트릭)
- Test 시에는 rounding

---
### Related Posts
MPEG 코덱의 미래 (2022)

### References
{% bibliography --cited %}