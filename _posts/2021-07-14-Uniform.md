---
toc: true
comments: true
layout: post
description: 딥러닝 압축 분야에서 엔트로피 가능도(entropy likelihood)를 계산하기 위해, 본래 미분이 불가능했던 이산적(discrete) 데이터를 연속적인(continuous) 근사를 통해 gradient를 계산할 수 있도록 하는 Uniform additive noise 방법에 대하여 서술한다.
categories: [markdown]
title: Uniform additive noise는 무엇이고, 어떻게 동작하는가?
---


## Introduction
- Data Compression에 관한 내용
- Shanon 표현법
- 엔트로피 최소화
- 기존 코덱 방법 동작
- 코딩 (CABAC) 등
- 'zero gradient'

## Traditional Video Codec
- 주파수 도메인 변환
- 양자화 & 라운딩
- QP 값
- RD-cost tradeoff (Perception tradeoff)
- RD plot

## Optimized End-to-end
- Review (GDN)
- Review (Uniform additive noise)
- Continuously-relaxed loss function
- '당황스럽게 잘 동작'
- 그림

## Deep Compression
STE

Model Compression through Entropy Penalized Reparameterization The model compression
work of Oktay et al. (2020) reparameterizes the model weights Θ into a latent space as Φ. The
latent weights are decoded by a learned function F, i.e. Θ = F(Φ). The latent weights Φ are
modeled as samples from a learned prior q, such that they can be entropy coded according to this
prior. To minimize the rate, i.e. length of the bit string resulting from entropy coding these latent
weights, a differentiable approximation of the self-information I(φ) = − log2
(q(φ)) of the latent
weights is penalized. The continuous Φ are quantized before being applied in the model, with
the straight-through estimator (Bengio et al., 2013) used to obtain surrogate gradients of the loss
function. Following Balle et al. (2017), uniform noise is added when learning the continuous prior ´
q(φ + u) where ui ∼ U(−
1
2
,
1
2
) ∀ i. This uniform noise is a stand-in for the quantization, and
results in a good approximation for the self-information through the negative log-likelihood of the
noised continuous latent weights. After training, the quantized weights Φ˜ are obtained by rounding,
Φ =˜ bΦe, and transmitted along with discrete probability tables obtained by integrating the density
over the quantization intervals. The continuous weights Φ and any parameters in q itself can then be
discarded
## Scalable Model Compression 

"Linear filters were parameterized using their discrete cosine transform (DCT) coefficients.
We found this to be slightly more effective in speeding up the convergence than discrete
Fourier transform (DFT) parameterization - ICLR'17"

(2부?)
## Implementation (code)
- tensorflow-compression
- CompressAI

## 코드의 동작법 (트릭)
- Test 시에는 rounding

---
### Related Posts
MPEG 코덱의 미래 (2022)

### References
{% bibliography --cited %}