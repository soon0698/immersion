---
toc: true
comments: true
layout: post
description: 단순한 multilayer perceptron (MLP)를 통해 visual representations를 근사하는 방법으로, 최근 여러 vision task에서 압도적인 성능으로 대표 방법이 되어가고 있는 'implicit neural representations' 계열의 대표 논문들에 대하여 서술한다.  
categories: [markdown]
title: Review - Implicit Neural Representations 
---


## Introduction

What are implicit neural representations?
Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds. Conventional signal representations are usually discrete - for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a continuous function that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to "write down" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.
- 직접적인 데이터(신호)를 전달한다. 

Why are they interesting?
Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have "infinite resolution" - they can be sampled at arbitrary spatial resolutions.

This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations.

Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the "convolutional neural network" equivalent of a neural network operating on images represented by implicit representations?


- 트위터 인용 (https://twitter.com/vincesitzmann/status/1343628606405271554)
- Implicit layer와는 다르다 (트위터 논쟁)
- coordinate-based MLPs -> Sharing하고 Generalize하는 CNN 기반이 아니라, 하나의 MLP에 overfitting하는 것.
- Photometry
- Depth map (Stereo)
- 기존의 문제 (Low-level 비전 / VAE)


## Nerf
- Multiview
- Photometry
- Depth map (Stereo)
- novel view synthesis 
- 다른 점 강조 (CNN은 공유 가중치, MLP은 fitting)


## 'Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains' NIPS'20
- 영상
- neural tangent kernel (NTK)
- positional encoding

## Implicit Neural Representations with Periodic Activation Functions
- Concurrent Work? (좀 결이 다름)


## Implementation (code)
- Code


---
### Related Posts
MPEG Immersive Video


### References
{% bibliography --cited %}