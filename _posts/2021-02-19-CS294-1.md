---
toc: true
comments: true
show_image: true
layout: post
description: 버클리대학에서 진행된 Deep Unsupervised Learning 강의 1을 요약한다.
categories: [markdown]
title: CS294-158-SP20 Deep Unsupervised Learning Spring 2020 - (1)
---

> 본 포스팅은 버클리대학에서 진행된 Deep Unsupervised Learning 강의를 요약하고자 만들어진 포스팅이다. 강의에 대한 모든 자료와 영상은 [웹사이트](https://sites.google.com/view/berkeley-cs294-158-sp20/home)에 공개되어 있으며, 강의자인 [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)은 [버클리 인공지능 연구 센터(BAIR)](https://bair.berkeley.edu/blog/?refresh=1)의 디렉터이기도 하다. 


# Introduction
---
<!--https://www.youtube.com/watch?v=V9Roouqfu-M&feature=youtu.be-->
## What is Unsupervised Learning?
<!-->
//REF : 대표 자료 조사
-->
Unsupervised Learning(비지도학습)을 정말 간단히 요약하자면, 가공되지 않은 데이터로부터 패턴을 학습하는 것이다.<!--Capturing rich patterns in raw data with deep networks in a label-free way--> 기존의 지도학습이 데이터에 대한 정답(라벨)이 미리 요구된다면, 비지도학습은 그런 과정이 필요없는 장점을 갖고 있다. 현재의 비지도학습 방법론은 크게 2가지 범주로 분류할 수 있다.

- 생성 모델(Generative Models): 미가공 데이터에 대한 분포를 재구성하는 방법<!--recreate raw data distribution-->

생성 모델은 학습 데이터의 확률 분포를 학습하게 된다. 일단 이 확률 분포를 학습하여 구성하고나면 기존의 데이터를 분포에 맞추어 생성할 수도 있으며, 분포의 재구성을 통해 새로운 데이터 역시 생성하거나 추론할 수 있다. 이에 대해서는 강의를 진행하며 세부적인 내용들을 다룰 것이다.<!--https://minsuksung-ai.tistory.com/12-->

-  자기지도학습(Self-supervised Learning): 의미있는 정보의 이해가 요구되는 task를 해결하도록 학습하는 방법 <!--"puzzle" task that require semantic understanding-->

자기지도학습은 특정 task를 해결하기 위한 표현을 추출하여 학습한다. 예를 들어 임의로 [0, 90, 180, 270]도씩 회전시킨 이미지를 입력 영상으로 제공하였을 때, 회전된 이미지가 원본 이미지가 되기 위해서는 다시 얼마나 회전시켜야 하는지에 대한 task를 정의하여 task에 필요한 표현을 학습하고자 한 시도가 있다. 이 시도는 매우 간단한 일처럼 보이지만, 사실은 자기지도학습에서 아주 중요하고 심오한 연구 주제로 밝혀진 바 있다. {% cite gidaris2018unsupervised %} <!--https://greeksharifa.github.io/self-supervised%20learning/2020/11/01/Self-Supervised-Learning/-->

## Ideal Intelligence
---

비지도학습이 각광받는 이유 중 하나로는 비지도학습이 이상적인 지능(Ideal Intelligence)의 형태에 가장 걸맞는 방법이라는 것이다. 그렇다면 이상적인 지능은 어떤 것을 갖추고 있어야 하는 것일까? 한마디로 정의하자면, 다음과 같다.

> "Ideal Intelligence" is all about compression (finding all patterns)

- 이상적인 지능은 결국 압축에 관한 것이다. 데이터에 대한 패턴을 찾는다는 것은 압축과 맞닿아있는 개념이다. **데이터에 내재된 패턴을 이해하고, 이를 더 간결한 형태로 바꿔쓰는 작업이기 때문이다.** 이러한 맥락으로 생각한다면, 다음과 같은 수학적 개념들이 제시될 수 있다.

> **Low Kolmogorov Complexity** (short description of raw data)

- Kolmogorov Complexity[^2]는 주어진 데이터를 생성할 수 있는 가장 짧은 형태의 표현은 무엇인가? 라는 개념을 제시한다. 가장 짧은 형태의 표현을 찾아낼 수 있다면 우리는 주어진 데이터를 보다 효율적으로 압축할 수 있게 된다.

> **Solomonoff Induction** (optimal inference)
<!--http://www.scholarpedia.org/article/Algorithmic_probability-->
- Solomonoff Induction은 Kolmogorov Complexity의 


---
## Related Posts

> Note: [Course Website](https://sites.google.com/view/berkeley-cs294-158-sp20/home)
**Instructor:** [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/) <br/>**Syllabus**<br/>**L1 (1/22) Introduction**<br/>L2 (1/29) Autoregressive Models<br/>L3 (2/5) Flow Models<br/>L4 (2/12) Latent Variable Models<br/>L5 (2/19) Implicit Models / Generative Adversarial Networks<br/>L6 (2/26) Implicit Models / Generative Adversarial Networks (ctd) + Final Project Discussion<br/>L7 (3/11) Self-Supervised Learning / Non-Generative Representation Learning<br/>L8 (3/18) Strengths and Weaknesses of Unsupervised Learning Methods Covered Thus Far<br/>L9 (4/1) Semi-Supervised Learning; Unsupervised Distribution Alignment<br/>L10 (4/8) Compression<br/>L11 (4/15) Language Models -- Guest Instructor: Alec Radford (OpenAI)<br/>L12 (4/29) Representation Learning in Reinforcement Learning

## References
{% bibliography --cited %}