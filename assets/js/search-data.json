{
  
    
        "post0": {
            "title": "TEST",
            "content": "some blue text. . %%latex . bf{This is a test} It allows latex $ alpha= beta$ and test: . begin{equation} alpha = beta end{equation} % this is a latex comment . %%latex begin{align} nabla cdot vec{ mathbf{E}} &amp; = 4 pi rho nabla times vec{ mathbf{E}} , + , frac1c , frac{ partial vec{ mathbf{B}}}{ partial t} &amp; = vec{ mathbf{0}} nabla cdot vec{ mathbf{B}} &amp; = 0 end{align} .",
            "url": "https://soon0698.github.io/immersion/jupyter/2021/11/01/test.html",
            "relUrl": "/jupyter/2021/11/01/test.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Review - Implicit Neural Representations",
            "content": "Introduction . coordinate-based MLPs | Photometry | Depth map (Stereo) | 기존의 문제 (Low-level 비전 / VAE) | . Nerf . Multiview | Photometry | Depth map (Stereo) | novel view synthesis | 다른 점 강조 (CNN은 공유 가중치, MLP은 fitting) | . ‘Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains’ NIPS’20 . 영상 | neural tangent kernel (NTK) | positional encoding | . Implicit Neural Representations with Periodic Activation Functions . Concurrent Work? (좀 결이 다름) | . Implementation (code) . Code | . . Related Posts . MPEG Immersive Video . References .",
            "url": "https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html",
            "relUrl": "/markdown/2021/08/15/Nerf.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Uniform additive noise는 무엇이고, 어떻게 동작하는가?",
            "content": "Introduction . Data Compression에 관한 내용 | Shanon 표현법 | 엔트로피 최소화 | 기존 코덱 방법 동작 | 코딩 (CABAC) 등 | ‘zero gradient’ | . Traditional Video Codec . 주파수 도메인 변환 | 양자화 &amp; 라운딩 | QP 값 | RD-cost tradeoff (Perception tradeoff) | RD plot | . Optimized End-to-end . Review (GDN) | Review (Uniform additive noise) | Continuously-relaxed loss function | ‘당황스럽게 잘 동작’ | 그림 | . Deep Compression . STE . Model Compression through Entropy Penalized Reparameterization The model compression work of Oktay et al. (2020) reparameterizes the model weights Θ into a latent space as Φ. The latent weights are decoded by a learned function F, i.e. Θ = F(Φ). The latent weights Φ are modeled as samples from a learned prior q, such that they can be entropy coded according to this prior. To minimize the rate, i.e. length of the bit string resulting from entropy coding these latent weights, a differentiable approximation of the self-information I(φ) = − log2 (q(φ)) of the latent weights is penalized. The continuous Φ are quantized before being applied in the model, with the straight-through estimator (Bengio et al., 2013) used to obtain surrogate gradients of the loss function. Following Balle et al. (2017), uniform noise is added when learning the continuous prior ´ q(φ + u) where ui ∼ U(− 1 2 , 1 2 ) ∀ i. This uniform noise is a stand-in for the quantization, and results in a good approximation for the self-information through the negative log-likelihood of the noised continuous latent weights. After training, the quantized weights Φ˜ are obtained by rounding, Φ =˜ bΦe, and transmitted along with discrete probability tables obtained by integrating the density over the quantization intervals. The continuous weights Φ and any parameters in q itself can then be discarded . Scalable Model Compression . “Linear filters were parameterized using their discrete cosine transform (DCT) coefficients. We found this to be slightly more effective in speeding up the convergence than discrete Fourier transform (DFT) parameterization - ICLR’17” . (2부?) . Implementation (code) . tensorflow-compression | CompressAI | . 코드의 동작법 (트릭) . Test 시에는 rounding | . . Related Posts . MPEG 코덱의 미래 (2022) . References .",
            "url": "https://soon0698.github.io/immersion/markdown/2021/07/14/Uniform.html",
            "relUrl": "/markdown/2021/07/14/Uniform.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "CS294-158-SP20 Deep Unsupervised Learning Spring 2020 - (1)",
            "content": "본 포스팅은 버클리대학에서 진행된 Deep Unsupervised Learning 강의를 요약하고자 만들어진 포스팅이다. 강의에 대한 모든 자료와 영상은 웹사이트에 공개되어 있으며, 강의자인 Pieter Abbeel은 버클리 인공지능 연구 센터(BAIR)의 디렉터이기도 하다. . Introduction . . What is Unsupervised Learning? . Unsupervised Learning(비지도학습)을 정말 간단히 요약하자면, 가공되지 않은 데이터로부터 패턴을 학습하는 것이다. 기존의 지도학습이 데이터에 대한 정답(라벨)이 미리 요구된다면, 비지도학습은 그런 과정이 필요없는 장점을 갖고 있다. 현재의 비지도학습 방법론은 크게 2가지 범주로 분류할 수 있다. . 생성 모델(Generative Models): 미가공 데이터에 대한 분포를 재구성하는 방법 | . 생성 모델은 학습 데이터의 확률 분포를 학습하게 된다. 일단 이 확률 분포를 학습하여 구성하고나면 기존의 데이터를 분포에 맞추어 생성할 수도 있으며, 분포의 재구성을 통해 새로운 데이터 역시 생성하거나 추론할 수 있다. 이에 대해서는 강의를 진행하며 세부적인 내용들을 다룰 것이다. . 자기지도학습(Self-supervised Learning): 의미있는 정보의 이해가 요구되는 task를 해결하도록 학습하는 방법 | . 자기지도학습은 특정 task를 해결하기 위한 표현을 추출하여 학습한다. 예를 들어 임의로 [0, 90, 180, 270]도씩 회전시킨 이미지를 입력 영상으로 제공하였을 때, 회전된 이미지가 원본 이미지가 되기 위해서는 다시 얼마나 회전시켜야 하는지에 대한 task를 정의하여 task에 필요한 표현을 학습하고자 한 시도가 있다. 이 시도는 매우 간단한 일처럼 보이지만, 사실은 자기지도학습에서 아주 중요하고 심오한 연구 주제로 밝혀진 바 있다. (Gidaris et al., 2018) . Ideal Intelligence . . 비지도학습이 각광받는 이유 중 하나로는 비지도학습이 이상적인 지능(Ideal Intelligence)의 형태에 가장 걸맞는 방법이라는 것이다. 그렇다면 이상적인 지능은 어떤 것을 갖추고 있어야 하는 것일까? 한마디로 정의하자면, 다음과 같다. . “Ideal Intelligence” is all about compression (finding all patterns) . 이상적인 지능은 결국 압축에 관한 것이다. 이 강의에서는 데이터에 내재된 패턴을 찾는 것을 압축과 맞닿아있는 개념으로 본다. 데이터에 내재된 패턴을 이해하고, 이를 더 간결한 형태로 바꿔쓰는 작업이기 때문이다. 이러한 맥락에서, 다음과 같은 수학적 개념들이 제시된다. | . Low Kolmogorov Complexity (short description of raw data) . Kolmogorov Complexity[^2]는 주어진 데이터를 생성할 수 있는 가장 짧은 형태의 표현은 무엇인가? 라는 개념을 제시한다. 가장 짧은 형태의 표현을 찾아낼 수 있다면 우리는 주어진 데이터를 보다 효율적으로 압축할 수 있게 된다. | . Solomonoff Induction (optimal inference) . Solomonoff Induction은 Kolmogorov Complexity의 | . . Related Posts . SyllabusL1 (1/22) IntroductionL2 (1/29) Autoregressive ModelsL3 (2/5) Flow ModelsL4 (2/12) Latent Variable ModelsL5 (2/19) Implicit Models / Generative Adversarial NetworksL6 (2/26) Implicit Models / Generative Adversarial Networks (ctd) + Final Project DiscussionL7 (3/11) Self-Supervised Learning / Non-Generative Representation LearningL8 (3/18) Strengths and Weaknesses of Unsupervised Learning Methods Covered Thus FarL9 (4/1) Semi-Supervised Learning; Unsupervised Distribution AlignmentL10 (4/8) CompressionL11 (4/15) Language Models – Guest Instructor: Alec Radford (OpenAI)L12 (4/29) Representation Learning in Reinforcement Learning . References . Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). Unsupervised Representation Learning by Predicting Image Rotations. |",
            "url": "https://soon0698.github.io/immersion/markdown/2021/02/19/CS294-1.html",
            "relUrl": "/markdown/2021/02/19/CS294-1.html",
            "date": " • Feb 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Research",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://soon0698.github.io/immersion/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://soon0698.github.io/immersion/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
      ,"page8": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://soon0698.github.io/immersion/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}