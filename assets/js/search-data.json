{
  
    
        "post0": {
            "title": "Review - Implicit Neural Representations",
            "content": "Introduction . What are implicit neural representations? Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds. Conventional signal representations are usually discrete - for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a continuous function that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to “write down” the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network. . 직접적인 데이터(신호)를 전달한다. | . Why are they interesting? Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have “infinite resolution” - they can be sampled at arbitrary spatial resolutions. . This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations. . Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What’s the “convolutional neural network” equivalent of a neural network operating on images represented by implicit representations? . 트위터 인용 (https://twitter.com/vincesitzmann/status/1343628606405271554) | Implicit layer와는 다르다 (트위터 논쟁) Implicit | Photometry | Depth map (Stereo) | 기존의 문제 (Low-level 비전 / VAE) | . Nerf . Multiview | Photometry | Depth map (Stereo) | novel view synthesis | 다른 점 강조 (CNN은 공유 가중치, MLP은 fitting) | . SIREN . https://twitter.com/vincesitzmann/status/1274119064797888513 https://github.com/bmild/nerf/issues/60 . ‘Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains’ NIPS’20 . 뒤이어 나온 후속논문인 약칭 Fourier Feature Networks에서는 입력 좌표에 대해 간단한 Fourier feature mapping을 해주는 것만으로도 MLP가 다양한 도메인에서 고주파 정보를 잘 습득하게 된다는 점을 밝힌 바 있다. 재미있게도 논문에서는 이렇게 MLP를 이용해 이산적인 신호를 연속함수로 근사하는 Implicit 방법에 대해 “coordinate-based” MLP라고 이름을 새로 붙여주었는데, 네트워크의 입력이 좌표값이 되고 예측값은 그 좌표에 해당하는 결과값(통상적으로 color, density, shape)이 된다는 점에서 더 직관적이고 합당한 이름이라고 생각한다. 논문에서는 제안하는 방법의 타당성을 위해 neural tangent kernel (NTK)의 개념을 활용해 이론적으로도 여러 내용을 제시하여 논문 자체에 담긴 수식들과 그 배경은 꽤 무겁게 느껴지지만, 제안하는 핵심 방법 자체는 코드 한 줄 변경만으로 이루어질 정도로 간단하다.(이는 부록으로 남겨둔다 http://jnwoo.com/post/10/) Coordinate-based MLP 방법론의 기본적인 형태는 입력 좌표 $v$에 대해 다음과 같다: . γ(v)=[cos⁡(2πvv),sin⁡(2πv)]T gamma( mathbf{v})=[ cos (2 pi mathbf{v} v), sin (2 pi mathbf{v})]^{ mathrm{T}}γ(v)=[cos(2πvv),sin(2πv)]T . 제안하는 방법에서는 우리가 예측할 신호에 대한 주파수 스펙트럼의 형태를 정확히 추정할 수 없기 때문에, 단순한 Gaussian 분포 $ mathcal{N} left(0, sigma^{2} right)$에 해당하는 $B$를 도입한다. 이 때, 표준편차는 하이퍼파라미터가 되며 뒤이어 설명하겠지만 안정적인 학습을 위해서는 이 하이퍼파라미터를 잘 결정해주는 게 매우 중요하다. 결과적으로 Fourier Feature Networks의 도입 형태는 다음과 같다: . γ(v)=[cos⁡(2πBv),sin⁡(2πBv)]T gamma( mathbf{v})=[ cos (2 pi mathbf{B} mathbf{v}), sin (2 pi mathbf{B} mathbf{v})]^{ mathrm{T}}γ(v)=[cos(2πBv),sin(2πBv)]T . (PE와의 비교) . 영상 | neural tangent kernel (NTK) | positional encoding | . Implicit Neural Representations with Periodic Activation Functions . Concurrent Work? (좀 결이 다름) | . Implementation (code) . Code | . . Related Posts . MPEG Immersive Video . References .",
            "url": "https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html",
            "relUrl": "/markdown/2021/08/15/Nerf.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Uniform additive noise는 무엇이고, 어떻게 동작하는가?",
            "content": "Introduction . Data Compression에 관한 내용 | Shanon 표현법 | 엔트로피 최소화 | 기존 코덱 방법 동작 | 코딩 (CABAC) 등 | ‘zero gradient’ | . Traditional Video Codec . 주파수 도메인 변환 | 양자화 &amp; 라운딩 | QP 값 | RD-cost tradeoff (Perception tradeoff) | RD plot | . Optimized End-to-end . Review (GDN) | Review (Uniform additive noise) | Continuously-relaxed loss function | ‘당황스럽게 잘 동작’ | 그림 | . Deep Compression . STE . Model Compression through Entropy Penalized Reparameterization The model compression work of Oktay et al. (2020) reparameterizes the model weights Θ into a latent space as Φ. The latent weights are decoded by a learned function F, i.e. Θ = F(Φ). The latent weights Φ are modeled as samples from a learned prior q, such that they can be entropy coded according to this prior. To minimize the rate, i.e. length of the bit string resulting from entropy coding these latent weights, a differentiable approximation of the self-information I(φ) = − log2 (q(φ)) of the latent weights is penalized. The continuous Φ are quantized before being applied in the model, with the straight-through estimator (Bengio et al., 2013) used to obtain surrogate gradients of the loss function. Following Balle et al. (2017), uniform noise is added when learning the continuous prior ´ q(φ + u) where ui ∼ U(− 1 2 , 1 2 ) ∀ i. This uniform noise is a stand-in for the quantization, and results in a good approximation for the self-information through the negative log-likelihood of the noised continuous latent weights. After training, the quantized weights Φ˜ are obtained by rounding, Φ =˜ bΦe, and transmitted along with discrete probability tables obtained by integrating the density over the quantization intervals. The continuous weights Φ and any parameters in q itself can then be discarded . Scalable Model Compression . “Linear filters were parameterized using their discrete cosine transform (DCT) coefficients. We found this to be slightly more effective in speeding up the convergence than discrete Fourier transform (DFT) parameterization - ICLR’17” . (2부?) . Implementation (code) . tensorflow-compression | CompressAI | . 코드의 동작법 (트릭) . Test 시에는 rounding | . . Related Posts . MPEG 코덱의 미래 (2022) . References .",
            "url": "https://soon0698.github.io/immersion/markdown/2021/07/14/Uniform.html",
            "relUrl": "/markdown/2021/07/14/Uniform.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "CS294-158-SP20 Deep Unsupervised Learning Spring 2020 - (1)",
            "content": "본 포스팅은 버클리대학에서 진행된 Deep Unsupervised Learning 강의를 요약하고자 만들어진 포스팅이다. 강의에 대한 모든 자료와 영상은 웹사이트에 공개되어 있으며, 강의자인 Pieter Abbeel은 버클리 인공지능 연구 센터(BAIR)의 디렉터이기도 하다. . Introduction . . What is Unsupervised Learning? . Unsupervised Learning(비지도학습)을 정말 간단히 요약하자면, 가공되지 않은 데이터로부터 패턴을 학습하는 것이다. 기존의 지도학습이 데이터에 대한 정답(라벨)이 미리 요구된다면, 비지도학습은 그런 과정이 필요없는 장점을 갖고 있다. 현재의 비지도학습 방법론은 크게 2가지 범주로 분류할 수 있다. . 생성 모델(Generative Models): 미가공 데이터에 대한 분포를 재구성하는 방법 | . 생성 모델은 학습 데이터의 확률 분포를 학습하게 된다. 일단 이 확률 분포를 학습하여 구성하고나면 기존의 데이터를 분포에 맞추어 생성할 수도 있으며, 분포의 재구성을 통해 새로운 데이터 역시 생성하거나 추론할 수 있다. 이에 대해서는 강의를 진행하며 세부적인 내용들을 다룰 것이다. . 자기지도학습(Self-supervised Learning): 의미있는 정보의 이해가 요구되는 task를 해결하도록 학습하는 방법 | . 자기지도학습은 특정 task를 해결하기 위한 표현을 추출하여 학습한다. 예를 들어 임의로 [0, 90, 180, 270]도씩 회전시킨 이미지를 입력 영상으로 제공하였을 때, 회전된 이미지가 원본 이미지가 되기 위해서는 다시 얼마나 회전시켜야 하는지에 대한 task를 정의하여 task에 필요한 표현을 학습하고자 한 시도가 있다. 이 시도는 매우 간단한 일처럼 보이지만, 사실은 자기지도학습에서 아주 중요하고 심오한 연구 주제로 밝혀진 바 있다. (Gidaris et al., 2018) . Ideal Intelligence . . 비지도학습이 각광받는 이유 중 하나로는 비지도학습이 이상적인 지능(Ideal Intelligence)의 형태에 가장 걸맞는 방법이라는 것이다. 그렇다면 이상적인 지능은 어떤 것을 갖추고 있어야 하는 것일까? 한마디로 정의하자면, 다음과 같다. . “Ideal Intelligence” is all about compression (finding all patterns) . 이상적인 지능은 결국 압축에 관한 것이다. 이 강의에서는 데이터에 내재된 패턴을 찾는 것을 압축과 맞닿아있는 개념으로 본다. 데이터에 내재된 패턴을 이해하고, 이를 더 간결한 형태로 바꿔쓰는 작업이기 때문이다. 이러한 맥락에서, 다음과 같은 수학적 개념들이 제시된다. | . Low Kolmogorov Complexity (short description of raw data) . Kolmogorov Complexity[^2]는 주어진 데이터를 생성할 수 있는 가장 짧은 형태의 표현은 무엇인가? 라는 개념을 제시한다. 가장 짧은 형태의 표현을 찾아낼 수 있다면 우리는 주어진 데이터를 보다 효율적으로 압축할 수 있게 된다. | . Solomonoff Induction (optimal inference) . Solomonoff Induction은 Kolmogorov Complexity의 | . . Related Posts . SyllabusL1 (1/22) IntroductionL2 (1/29) Autoregressive ModelsL3 (2/5) Flow ModelsL4 (2/12) Latent Variable ModelsL5 (2/19) Implicit Models / Generative Adversarial NetworksL6 (2/26) Implicit Models / Generative Adversarial Networks (ctd) + Final Project DiscussionL7 (3/11) Self-Supervised Learning / Non-Generative Representation LearningL8 (3/18) Strengths and Weaknesses of Unsupervised Learning Methods Covered Thus FarL9 (4/1) Semi-Supervised Learning; Unsupervised Distribution AlignmentL10 (4/8) CompressionL11 (4/15) Language Models – Guest Instructor: Alec Radford (OpenAI)L12 (4/29) Representation Learning in Reinforcement Learning . References . Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). Unsupervised Representation Learning by Predicting Image Rotations. |",
            "url": "https://soon0698.github.io/immersion/markdown/2021/02/19/CS294-1.html",
            "relUrl": "/markdown/2021/02/19/CS294-1.html",
            "date": " • Feb 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Research",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://soon0698.github.io/immersion/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://soon0698.github.io/immersion/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
      ,"page8": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://soon0698.github.io/immersion/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}