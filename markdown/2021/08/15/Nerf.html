<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Review - Implicit Neural Representations | Immersion</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Review - Implicit Neural Representations" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="단순한 multilayer perceptrons (MLPs)를 통해 대부분의 signal representations를 근사하는 방법으로, 최근 여러 vision task에서 압도적인 성능으로 대표 방법이 되어가고 있는 ‘implicit neural representations’ 계열의 대표 논문들에 대하여 서술한다." />
<meta property="og:description" content="단순한 multilayer perceptrons (MLPs)를 통해 대부분의 signal representations를 근사하는 방법으로, 최근 여러 vision task에서 압도적인 성능으로 대표 방법이 되어가고 있는 ‘implicit neural representations’ 계열의 대표 논문들에 대하여 서술한다." />
<link rel="canonical" href="https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html" />
<meta property="og:url" content="https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html" />
<meta property="og:site_name" content="Immersion" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-08-15T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html","headline":"Review - Implicit Neural Representations","dateModified":"2021-08-15T00:00:00-05:00","datePublished":"2021-08-15T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://soon0698.github.io/immersion/markdown/2021/08/15/Nerf.html"},"description":"단순한 multilayer perceptrons (MLPs)를 통해 대부분의 signal representations를 근사하는 방법으로, 최근 여러 vision task에서 압도적인 성능으로 대표 방법이 되어가고 있는 ‘implicit neural representations’ 계열의 대표 논문들에 대하여 서술한다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/immersion/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://soon0698.github.io/immersion/feed.xml" title="Immersion" /><link rel="shortcut icon" type="image/x-icon" href="/immersion/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/immersion/">Immersion</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/immersion/about/">About</a><a class="page-link" href="/immersion/research/">Research</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Review - Implicit Neural Representations</h1><p class="page-description">단순한 multilayer perceptrons (MLPs)를 통해 대부분의 signal representations를 근사하는 방법으로, 최근 여러 vision task에서 압도적인 성능으로 대표 방법이 되어가고 있는 'implicit neural representations' 계열의 대표 논문들에 대하여 서술한다.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-08-15T00:00:00-05:00" itemprop="datePublished">
        Aug 15, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a>
<ul>
<li class="toc-entry toc-h3"><a href="#what-is-implicit-neural-representations">What is implicit neural representations?</a></li>
<li class="toc-entry toc-h3"><a href="#why-are-they-interesting">Why are they interesting?</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#siren">SIREN</a></li>
<li class="toc-entry toc-h2"><a href="#fourier-features-let-networks-learn-high-frequency-functions-in-low-dimensional-domains-nips20">‘Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains’ NIPS’20</a></li>
<li class="toc-entry toc-h2"><a href="#nerf-여기선-간단히-설명하고-다른-포스트로-특집---코드리뷰도-하고-연관논문-소개도-하고">Nerf (여기선 간단히 설명하고, 다른 포스트로 특집.   코드리뷰도 하고, 연관논문 소개도 하고.)</a></li>
<li class="toc-entry toc-h2"><a href="#implementation-code">Implementation (code)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#related-posts">Related Posts</a></li>
<li class="toc-entry toc-h3"><a href="#references">References</a></li>
</ul>
</li>
</ul><p>https://lifeisforu.tistory.com/539 What is Implicit modelling?</p>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<h3 id="what-is-implicit-neural-representations">
<a class="anchor" href="#what-is-implicit-neural-representations" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is implicit neural representations?</h3>
<p>우리가 처리하는 대부분의 신호는 픽셀 단위의 이미지, 샘플링된 오디오 신호, 그리고 3차원 데이터라 하더라도 통상적으로 복셀(voxel), 포인트 클라우드(point cloud), 메쉬(mesh) 등으로 처리하는게 용이하듯이, 이산적인 신호 표현을 많이 다루게 된다. Implicit Neural Representations 연구에서는 이러한 이산적 신호 표현들을 연속적인 함수로 취급해 다루고자 한다. 그 방법 중 하나는 ‘신호를 mapping할 연속적인 도메인’을 입력으로 받으면, ‘도메인에 해당하는 signal의 값’을 출력하는 함수를 찾는 것이다. 간단히 예를 들어, 이미지를 다루고자 한다면 ‘좌표’를 입력받아서 ‘좌표에 해당하는 이미지의 RGB 값’을 출력하는 함수를 만드는 것이다. 얼핏 생각해보면 이러한 함수는 직접 수식을 일일히 도입하는 수치적 방식으론 풀어낼 수 없는 문제이지만, 뉴럴 네트워크의 강력한 근사능력을 활용하여 ‘그럴듯한’ implicit 함수를 만들어낼 수 있는 시기가 도래한 것이다.</p>

<h3 id="why-are-they-interesting">
<a class="anchor" href="#why-are-they-interesting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why are they interesting?</h3>
<p>그렇다면 굳이 그 어려운 문제를 풀어야 하는 이유는 무엇일까? 이산적인 신호를 연속적으로 다루면 어떤 점에서 좋을까? 먼저 한 가지 예시 논문을 들어보자. (Occupancy Networks: Learning 3D Reconstruction in Function Space)에서는 3차원 데이터를 뉴럴 네트워크에 학습해 복원하고자 하는 task를 다룬다. 가장 먼저 직면하는 한 가지 문제점은 3차원 데이터에 대한 메모리와 계산량 문제이다. 복셀(voxel) 표현을 이용할 경우, 한 면이 256개의 복셀로 이루어진 작은 데이터를 표현하는데에도 $256^{3}$개의 그리드가 필요하며 메모리 측면에서 효율적이라고 볼 수 없다. 포인트 클라우드(point cloud) 표현은 현재까지도 여러 많은 연구가 진행되고 있지만 여전히 데이터 특성상 다루기 어렵고 물체의 boundary 등 structure 정보가 모호한 단점을 갖고 있다. 메쉬(mesh) 표현은 그 특성상 데이터의 표현형이 제한(특히 사람의 얼굴 등)되어있기 때문에 범용적으로 사용되기가 어렵다.</p>

<p>이 논문에서는 그렇다면 뉴럴 네트워크가 이 단점들을 모두 해결하는 ‘제 4의 표현을 찾아내줄 수 있을까?’ 라는 질문을 던지고, continuous 3D occupancy function이 실제로 그럴 수 있다는 것을 보이며 occupancy function을 제안한다.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mn>3</mn></msup><mo>→</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f_{\theta}: \mathbb{R}^{3} \rightarrow[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>

<p>그 함수의 동작방식은 충격적으로 단순한데, 3차원 좌표값을 네트워크에게 입력하여 해당 위치의 occupancy probability를 출력하게 하는 것이 전부이다. 쉽게 말해서, 입력 좌표에 해당하는 위치가 비어있는지 아닌지를 classification하는 쿼리를 학습과정 내내 계속해서 던지는 것이다. 이렇게 단순하게 도식화되면 갑자기 재미난 일들이 일어나기 시작한다. <strong>우선, 입력값인 3차원 좌표 차원 이상의 메모리를 필요로 하지 않게 된다.</strong> 게다가 입력 좌표값은 어디까지나 연속적인 도메인이기 때문에, 해상도를 늘리고 줄여도 Implicit 함수의 출력이 어느정도 일관성있게 유지되는 모습을 보인다. 이러한 특징 덕분에 implicit representations는 “infinite resolution”라고도 불린다.</p>

<p>‘3D 도메인에 강하다’ (memory efficient), https://autonomousvision.github.io/occupancy-networks/
(Learning Shape Templates with Structured Implicit Functions)
Vincent Sitzmann[github reference]</p>

<p><img src="https://soon0698.github.io/Immersion/images/1.png" alt="">
<img src="https://soon0698.github.io/Immersion/images/2.png" alt="" width="80%" height="80%"></p>

<ul>
  <li>트위터 인용 (https://twitter.com/vincesitzmann/status/1343628606405271554)</li>
  <li>Implicit layer와는 다르다 (트위터 논쟁)
Implicit</li>
  <li>Photometry</li>
  <li>Depth map (Stereo)</li>
  <li>기존의 문제 (Low-level 비전 / VAE)</li>
</ul>

<h2 id="siren">
<a class="anchor" href="#siren" aria-hidden="true"><span class="octicon octicon-link"></span></a>SIREN</h2>
<p>https://twitter.com/vincesitzmann/status/1274119064797888513
https://github.com/bmild/nerf/issues/60</p>

<p>‘Implicit neural representation’ 말 자체를 정의한 논문</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mrow><mo fence="true">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="normal">Φ</mi><mo separator="true">,</mo><msub><mi mathvariant="normal">∇</mi><mi mathvariant="bold">x</mi></msub><mi mathvariant="normal">Φ</mi><mo separator="true">,</mo><msubsup><mi mathvariant="normal">∇</mi><mi mathvariant="bold">x</mi><mn>2</mn></msubsup><mi mathvariant="normal">Φ</mi><mo separator="true">,</mo><mo>…</mo><mo fence="true">)</mo></mrow><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mspace width="1em"></mspace><mi mathvariant="normal">Φ</mi><mo>:</mo><mi mathvariant="bold">x</mi><mo>↦</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F\left(\mathbf{x}, \Phi, \nabla_{\mathbf{x}} \Phi, \nabla_{\mathbf{x}}^{2} \Phi, \ldots\right)=0, \quad \Phi: \mathbf{x} \mapsto \Phi(\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2141179999999998em;vertical-align:-0.35001em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Φ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.161108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">Φ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">x</span></span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord">Φ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord">Φ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.522em;vertical-align:-0.011em;"></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">↦</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span>

<h2 id="fourier-features-let-networks-learn-high-frequency-functions-in-low-dimensional-domains-nips20">
<a class="anchor" href="#fourier-features-let-networks-learn-high-frequency-functions-in-low-dimensional-domains-nips20" aria-hidden="true"><span class="octicon octicon-link"></span></a>‘Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains’ NIPS’20</h2>
<p>뒤이어 나온 후속논문인 약칭 Fourier Feature Networks에서는 입력 좌표에 대해 간단한 Fourier feature mapping을 해주는 것만으로도 MLP가 다양한 도메인에서 고주파 정보를 잘 습득하게 된다는 점을 밝힌 바 있다. 재미있게도 논문에서는 이렇게 MLP를 이용해 이산적인 신호를 연속함수로 근사하는 Implicit 방법에 대해 “coordinate-based” MLP라고 이름을 새로 붙여주었는데, 네트워크의 입력이 좌표값이 되고 예측값은 그 좌표에 해당하는 결과값(통상적으로 color, density-&gt;NeRF, shape-&gt;occupany network)이 된다는 점에서 더 직관적이고 합당한 이름이라고 생각한다. 논문에서는 제안하는 방법의 타당성을 위해 neural tangent kernel (NTK)의 개념을 활용해 이론적으로도 여러 내용을 제시하여 논문 자체에 담긴 수식들과 그 배경은 꽤 무겁게 느껴지지만, 제안하는 핵심 방법 자체는 코드 한 줄 변경만으로 이루어질 정도로 간단하다.(이는 부록으로 남겨둔다 http://jnwoo.com/post/10/) Coordinate-based MLP 방법론의 기본적인 형태는 입력 좌표 $v$에 대해 다음과 같다:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>π</mi><mi mathvariant="bold">v</mi><mi>v</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>π</mi><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">]</mo><mi mathvariant="normal">T</mi></msup></mrow><annotation encoding="application/x-tex">\gamma(\mathbf{v})=[\cos (2 \pi \mathbf{v} v), \sin (2 \pi \mathbf{v})]^{\mathrm{T}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mop">cos</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span></span></span></span></span></span>

<p>제안하는 방법에서는 우리가 예측할 신호에 대한 주파수 스펙트럼의 형태를 정확히 추정할 수 없기 때문에, 단순한 Gaussian 분포 $\mathcal{N}\left(0, \sigma^{2}\right)$에 해당하는 $B$를 도입한다. 이 때, 표준편차는 하이퍼파라미터가 되며 뒤이어 설명하겠지만 안정적인 학습을 위해서는 이 하이퍼파라미터를 잘 결정해주는 게 매우 중요하다. 결과적으로 Fourier Feature Networks의 도입 형태는 다음과 같다:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>π</mi><mi mathvariant="bold">B</mi><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>π</mi><mi mathvariant="bold">B</mi><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">]</mo><mi mathvariant="normal">T</mi></msup></mrow><annotation encoding="application/x-tex">\gamma(\mathbf{v})=[\cos (2 \pi \mathbf{B} \mathbf{v}), \sin (2 \pi \mathbf{B} \mathbf{v})]^{\mathrm{T}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mop">cos</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mord"><span class="mord mathbf">B</span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mord"><span class="mord mathbf">B</span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span></span></span></span></span></span>

<p>(PE와의 비교)</p>

<ul>
  <li>영상</li>
  <li>neural tangent kernel (NTK)</li>
  <li>positional encoding</li>
</ul>

<h2 id="nerf-여기선-간단히-설명하고-다른-포스트로-특집---코드리뷰도-하고-연관논문-소개도-하고">
<a class="anchor" href="#nerf-%EC%97%AC%EA%B8%B0%EC%84%A0-%EA%B0%84%EB%8B%A8%ED%9E%88-%EC%84%A4%EB%AA%85%ED%95%98%EA%B3%A0-%EB%8B%A4%EB%A5%B8-%ED%8F%AC%EC%8A%A4%ED%8A%B8%EB%A1%9C-%ED%8A%B9%EC%A7%91---%EC%BD%94%EB%93%9C%EB%A6%AC%EB%B7%B0%EB%8F%84-%ED%95%98%EA%B3%A0-%EC%97%B0%EA%B4%80%EB%85%BC%EB%AC%B8-%EC%86%8C%EA%B0%9C%EB%8F%84-%ED%95%98%EA%B3%A0" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nerf (여기선 간단히 설명하고, 다른 포스트로 특집.   코드리뷰도 하고, 연관논문 소개도 하고.)</h2>
<ul>
  <li>Multiview</li>
  <li>Photometry</li>
  <li>Depth map (Stereo)</li>
  <li>novel view synthesis</li>
  <li>다른 점 강조 (CNN은 공유 가중치, MLP은 fitting)</li>
</ul>

<h2 id="implementation-code">
<a class="anchor" href="#implementation-code" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation (code)</h2>
<ul>
  <li>Code</li>
</ul>

<hr>
<h3 id="related-posts">
<a class="anchor" href="#related-posts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Related Posts</h3>
<p>MPEG Immersive Video</p>

<h3 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h3>
<ol class="bibliography"></ol>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="soon0698/immersion"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/immersion/markdown/2021/08/15/Nerf.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/immersion/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/immersion/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/immersion/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li><a class="u-email" href="mailto:soon0698@gmail.com">soon0698@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Think hard. My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/soon0698" title="soon0698"><svg class="svg-icon grey"><use xlink:href="/immersion/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
